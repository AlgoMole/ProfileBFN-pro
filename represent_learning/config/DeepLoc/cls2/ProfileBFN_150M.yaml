setting:
  seed: 20000812
  ROOTDIR: './'
  os_environ:
    WANDB_API_KEY: ~
    WANDB_RUN_ID: ~
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    MASTER_ADDR: localhost
    MASTER_PORT: 12315
    WORLD_SIZE: 1
    NODE_RANK: 0
  wandb_config:
    project: DeepLoc_cls2
    name: ProfileBFN_150M
  tensorboard_config:
    save_dir: "./tensorboard/"
    name: DeepLoc_cls2
    version: DeepLoc_cls2
    log_graph: False
    default_hp_metric: True
    prefix: ""

model_checkpoint:
  monitor: "valid/valid_acc" # name of the logged metric which determines when model is improving
  mode: "max" # "max" means higher metric value is better, can be also "min"
  save_top_k: 1 # save k best models (determined by above metric)
  save_last: True # additionaly always save model from last epoch
  verbose: True
  dirpath: ""
  filename: "valid_acc_{valid/valid_acc:.5f}"
  auto_insert_metric_name: False
  every_n_train_steps: 0
  every_n_epochs: 1

model:
#    Which model to use
  model_py_path: bfn/bfn_classification_model
  kwargs:
#    Arguments to initialize the specific class
    net_name: checkpoints/ProfileBFN/ProfileBFN_150M
    load_pretrained: True
    num_labels: 2
    load_prev_scheduler: True
    save_top_k: 1

    freeze_backbone: 0
    dropout: 0.0
    use_lora: 0


#    Arguments to initialize the basic class AbstractModel
  lr_scheduler_kwargs:
    last_epoch: -1
    init_lr: 2.5e-5
#    Weather to use this scheduler or not
    on_use: false

  optimizer_kwargs:
    betas: [0.9, 0.98]
    weight_decay: 0.01

  save_path: checkpoints/ProfileBFN/DeepLoc/cls2_ProfileBFN_150M


dataset:
#    Arguments to initialize the basic class LMDBDataset
  dataset_py_path: bfn/bfn_classification_dataset
  dataloader_kwargs:
    batch_size: 4
    num_workers: 8

  train_lmdb: data/LMDB/DeepLoc/cls2/normal/train
  valid_lmdb: data/LMDB/DeepLoc/cls2/normal/test
  test_lmdb: “”
#    Arguments to initialize the specific class
  kwargs:
    tokenizer: checkpoints/ProfileBFN/ProfileBFN_150M

#  Arguments to initialize Pytorch Lightning Trainer
Trainer:
  max_epochs: 100
  log_every_n_steps: 1
  strategy:
    find_unused_parameters: True
  logger: wandb
  #enable_checkpointing: false
  val_check_interval: 1.0
  accelerator: gpu
  devices: 8
  num_nodes: 1
  accumulate_grad_batches: 2
  precision: 16
  num_sanity_val_steps: 0
  enable_progress_bar: true