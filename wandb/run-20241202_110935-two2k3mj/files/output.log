Using 16bit None Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:92: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).
  rank_zero_warn(
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  | Name       | Type                         | Params
------------------------------------------------------------
0 | model      | BFNForSequenceClassification | 653 M
1 | train_aupr | AveragePrecision             | 0
2 | valid_aupr | AveragePrecision             | 0
3 | test_aupr  | AveragePrecision             | 0
------------------------------------------------------------
653 M     Trainable params
0         Non-trainable params
653 M     Total params
1,306.190 Total estimated model params size (MB)
Traceback (most recent call last):
  File "/AIRvePFS/ai4science/users/yupei/ProfileBFN-pro/represent_learning/training.py", line 93, in <module>
    main()
  File "/AIRvePFS/ai4science/users/yupei/ProfileBFN-pro/represent_learning/training.py", line 88, in main
    run(config)
  File "/AIRvePFS/ai4science/users/yupei/ProfileBFN-pro/represent_learning/training.py", line 30, in run
    trainer.fit(model=model, datamodule=data_module)#, ckpt_path=config.model.save_path)
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 36, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 88, in launch
    return function(*args, **kwargs)
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1112, in _run
    results = self._run_stage()
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1191, in _run_stage
    self._run_train()
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1214, in _run_train
    self.fit_loop.run()
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 194, in run
    self.on_run_start(*args, **kwargs)
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 206, in on_run_start
    self.trainer.reset_train_dataloader(self.trainer.lightning_module)
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1529, in reset_train_dataloader
    self.train_dataloader = self._data_connector._request_dataloader(RunningStage.TRAINING)
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py", line 446, in _request_dataloader
    dataloader = source.dataloader()
  File "/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py", line 524, in dataloader
    return method()
  File "/AIRvePFS/ai4science/users/yupei/ProfileBFN-pro/represent_learning/dataset/lmdb_dataset.py", line 80, in train_dataloader
    return self._dataloader("train")
  File "/AIRvePFS/ai4science/users/yupei/ProfileBFN-pro/represent_learning/dataset/lmdb_dataset.py", line 74, in _dataloader
    dataset._init_lmdb(lmdb_path)
  File "/AIRvePFS/ai4science/users/yupei/ProfileBFN-pro/represent_learning/dataset/lmdb_dataset.py", line 47, in _init_lmdb
    self.env = lmdb.open(path, lock=False, map_size=_10TB)
FileNotFoundError: [Errno 2] No such file or directory: './LMDB/GO/AF2/CC/normal/train'