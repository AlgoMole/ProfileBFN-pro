Using 16bit None Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:92: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).
  rank_zero_warn(
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  | Name       | Type                         | Params
------------------------------------------------------------
0 | model      | BFNForSequenceClassification | 653 M
1 | train_aupr | AveragePrecision             | 0
2 | valid_aupr | AveragePrecision             | 0
3 | test_aupr  | AveragePrecision             | 0
------------------------------------------------------------
653 M     Trainable params
0         Non-trainable params
653 M     Total params
1,306.190 Total estimated model params size (MB)
DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 381
/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('epoch', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/root/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
Traceback (most recent call last):
  File "/AIRvePFS/ai4science/users/yupei/ProfileBFN-pro/represent_learning/training.py", line 93, in <module>
    main()
  File "/AIRvePFS/ai4science/users/yupei/ProfileBFN-pro/represent_learning/training.py", line 88, in main
    run(config)
  File "/AIRvePFS/ai4science/users/yupei/ProfileBFN-pro/represent_learning/training.py", line 40, in run
    model.load_checkpoint(os.path.join(model.save_path, 'best.ckpt'))
  File "/AIRvePFS/ai4science/users/yupei/ProfileBFN-pro/represent_learning/model/abstract_model.py", line 169, in load_checkpoint
    state_dict = torch.load(from_checkpoint, map_location=self.device)
  File "/root/miniconda3/lib/python3.9/site-packages/torch/serialization.py", line 771, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/root/miniconda3/lib/python3.9/site-packages/torch/serialization.py", line 270, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/root/miniconda3/lib/python3.9/site-packages/torch/serialization.py", line 251, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'checkpoints/GO/CC_trainGO/CC_BFN/best.ckpt'
Epoch 0:   0%|                                                                                                       | 12/29575 [00:38<26:18:37,  3.20s/it, loss=0.694, v_num=2l4i, train/train_loss=0.694, learning_rate=1.5e-5, epoch=0.000]